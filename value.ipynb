{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn import Neuron, Layer, MLP\n",
    "from value import Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "    # builds set of all nodes and edges in a graph\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_graph(root):\n",
    "    # builds a digraph based on the does and edges produced by trace, adds 'ghost' nodes for operations\n",
    "    graph = Digraph(format='svg', graph_attr={'rankdir': 'LR'})\n",
    "\n",
    "    nodes, edges = trace(root)\n",
    "\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        # for all values in the graph create a rectangular record node for it\n",
    "        graph.node(name = uid, label = \"{ %s | data %.2f | grad %.2f }\" % (n.label, n.data, n.grad), shape='record')\n",
    "        if n._op:\n",
    "            # if this value is the result of an operation, add a node for it\n",
    "            graph.node(name= uid + n._op, label = n._op)\n",
    "            # and connect this node to it\n",
    "            graph.edge(uid + n._op, uid)\n",
    "        \n",
    "    for n1, n2 in edges:\n",
    "        # connect n1 to the op node of n2\n",
    "        graph.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "        \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a single neuron manually:\n",
    "```tanh( (x1 * w1) + (x2 * w2) + b )```\n",
    "\n",
    "```x1, x2```: Inputs\n",
    "\n",
    "```w1, w2```: Weights\n",
    "\n",
    "```b```: Bias\n",
    "\n",
    "```tanh()```: Activation function $$tanh(x) = {e^{2x} - 1 \\over e^{2x} + 1}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "\n",
    "x1w1 = x1*w1; x1w1.label='x1w1'  # x1 * w1\n",
    "x2w2 = x2*w2; x2w2.label='x2w2'  # x2 * w2\n",
    "\n",
    "x1w1x2w2 = x1w1+x2w2; x1w1x2w2.label='x1w1x2w2' #  x1*w1 + x2*w2\n",
    "\n",
    "n = x1w1x2w2 + b; n.label='n'\n",
    "# -----\n",
    "e = (2 * n).exp(); e.label='e'\n",
    "o = (e - 1)/(e + 1); o.label='out'\n",
    "# -----\n",
    "o.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Neuron in Pytorch\n",
    "The exact same implementation as above, but this time using Pytorch\n",
    "\n",
    "Note: the ```.double()``` is used to match the default dtype from our own implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071066904050358\n",
      "---\n",
      "x2 0.5000001283844369\n",
      "w2 0.0\n",
      "x1 -1.5000003851533106\n",
      "w1 1.0000002567688737\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x1 = torch.Tensor([2.0]).double();                  x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double();                  x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double();                 w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double();                  w2.requires_grad = True\n",
    "b = torch.Tensor([6.8813735870195432]).double();    b.requires_grad = True\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = torch.tanh(n)\n",
    "\n",
    "print(o.data.item())\n",
    "o.backward()\n",
    "\n",
    "print('---')\n",
    "print('x2', x2.grad.item())\n",
    "print('w2', w2.grad.item())\n",
    "print('x1', x1.grad.item())\n",
    "print('w1', w1.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Multi-Layer Perceptron from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "nin = 3 # size of input layers\n",
    "nouts = [4,4,1] # size out all following layers\n",
    "n = MLP(nin,nouts)\n",
    "\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] #deisred targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the MLP \n",
    "This is done using gradient descent with the root mean squared error as the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:9.997190507132244e-05 achieved in 2964 iteration\n",
      "predicted values:[0.9932045828630143, -0.9950121091188823, -0.9866973629839282, 0.9876764161383002]\n"
     ]
    }
   ],
   "source": [
    "# --- params:---\n",
    "error = .0001      # target error\n",
    "max_iter = 10000    # max iterations\n",
    "h = 0.1            # step size\n",
    "# --------------\n",
    "\n",
    "loss = 1.0\n",
    "nn = 0\n",
    "while loss > error and max_iter > nn:\n",
    "    # nudge parameters until within the allowed error\n",
    "    ypred = [n(x) for x in xs] # calculate outputs\n",
    "    loss = sum([(yout-ytrue)**2 for yout, ytrue in zip(ys, ypred)])/len(ys) # calculate error\n",
    "\n",
    "    for p in n.parameters():\n",
    "        # reset gradients to 0 between iterations\n",
    "        p.grad = 0.0\n",
    "    \n",
    "    loss.backward() # backpropagate gradients\n",
    "\n",
    "    for p in n.parameters():\n",
    "        # nudge parameters in opposite direction of gradient\n",
    "        p.data += -h * p.grad\n",
    "    \n",
    "    nn += 1\n",
    "\n",
    "print(f'loss:{loss.data} achieved in {nn} iteration')\n",
    "print(f'predicted values:{[y.data for y in ypred]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
